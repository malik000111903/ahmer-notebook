{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Handling Missing Values in a Pandas DataFrame\n",
    "\n",
    "Missing values in Pandas are represented as `NaN`. We can identify and handle them using various methods:\n",
    "\n",
    "#### **Identifying Missing Values**\n",
    "1. `df.isnull()` → Returns a DataFrame showing `True` for missing values.\n",
    "2. `df.isnull().sum()` → Counts missing values per column.\n",
    "\n",
    "#### **Handling Missing Values**\n",
    "1. **Removing Missing Data**\n",
    "   - `df.dropna()` → Removes rows with missing values.\n",
    "   - `df.dropna(axis=1)` → Removes columns with missing values.\n",
    "\n",
    "2. **Filling Missing Data**\n",
    "   - `df.fillna(value)` → Replaces missing values with a specific value.\n",
    "   - `df['column'].fillna(df['column'].mean())` → Replaces missing values with the column's mean (for numerical data).\n",
    "   - `df.fillna(method='ffill')` → Forward fill (propagates last valid value).\n",
    "   - `df.fillna(method='bfill')` → Backward fill (uses next valid value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Name': ['Ali', 'Ayesha', 'Hassan', None, 'Sara'],\n",
    "    'Age': [25, None, 30, 28, None],\n",
    "    'City': ['Karachi', 'Lahore', None, 'Islamabad', 'Quetta']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identifying missing values\n",
    "print(\"Missing Values in DataFrame:\")\n",
    "print(df.isnull())  # Shows True for missing values\n",
    "\n",
    "print(\"\\nCount of Missing Values per Column:\")\n",
    "print(df.isnull().sum())  # Count missing values per column\n",
    "\n",
    "# Handling missing values\n",
    "\n",
    "# Removing rows with missing values\n",
    "df_dropped_rows = df.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\")\n",
    "print(df_dropped_rows)\n",
    "\n",
    "# Removing columns with missing values\n",
    "df_dropped_cols = df.dropna(axis=1)\n",
    "print(\"\\nDataFrame after dropping columns with missing values:\")\n",
    "print(df_dropped_cols)\n",
    "\n",
    "# Filling missing values with a specific value\n",
    "df_filled = df.fillna(\"Unknown\")\n",
    "print(\"\\nDataFrame after filling missing values with 'Unknown':\")\n",
    "print(df_filled)\n",
    "\n",
    "# Filling missing numerical values with the column's mean\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "print(\"\\nDataFrame after filling missing 'Age' values with mean:\")\n",
    "print(df)\n",
    "\n",
    "# Forward Fill\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(\"\\nDataFrame after Forward Fill:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# Backward Fill\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(\"\\nDataFrame after Backward Fill:\")\n",
    "print(df_bfill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Imputation, and Why is it Useful in Dealing with Missing Data?\n",
    "\n",
    "**Imputation** is the process of replacing missing values in a dataset with estimated values instead of removing them.\n",
    "\n",
    "#### **Why is Imputation Useful?**\n",
    "- **Prevents Data Loss**: Removing missing values can lead to loss of important information.\n",
    "- **Maintains Data Integrity**: Helps preserve the dataset size and structure.\n",
    "- **Improves Model Performance**: In machine learning, missing data can lead to bias or incorrect predictions.\n",
    "\n",
    "#### **Common Imputation Techniques**\n",
    "1. **Mean/Median/Mode Imputation** (for numerical data)\n",
    "   - Mean: Replaces missing values with the average of available data.\n",
    "   - Median: Uses the middle value of the dataset.\n",
    "   - Mode: Uses the most frequent value.\n",
    "\n",
    "2. **Forward/Backward Fill** (for time series data)\n",
    "   - **Forward Fill (`ffill`)**: Uses the previous row's value.\n",
    "   - **Backward Fill (`bfill`)**: Uses the next row's value.\n",
    "\n",
    "3. **Predictive Imputation** (Advanced)\n",
    "   - Uses machine learning models (e.g., KNN, regression) to predict missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Name': ['Ali', 'Ayesha', 'Hassan', None, 'Sara'],\n",
    "    'Age': [25, None, 30, 28, None],\n",
    "    'City': ['Karachi', 'Lahore', None, 'Islamabad', 'Quetta']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean Imputation (Filling missing Age values with mean)\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "print(\"\\nDataFrame after Mean Imputation for 'Age':\")\n",
    "print(df)\n",
    "\n",
    "# Median Imputation\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "print(\"\\nDataFrame after Median Imputation for 'Age':\")\n",
    "print(df)\n",
    "\n",
    "# Mode Imputation for categorical data (Filling missing 'City' values with mode)\n",
    "df['City'] = df['City'].fillna(df['City'].mode()[0])\n",
    "print(\"\\nDataFrame after Mode Imputation for 'City':\")\n",
    "print(df)\n",
    "\n",
    "# Forward Fill\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(\"\\nDataFrame after Forward Fill:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# Backward Fill\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(\"\\nDataFrame after Backward Fill:\")\n",
    "print(df_bfill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Can You Encode Categorical Variables in a Pandas DataFrame?\n",
    "\n",
    "**Categorical variables** are non-numeric variables that represent categories, such as `\"Male\"`/`\"Female\"` or `\"Red\"`/`\"Blue\"`/`\"Green\"`. Since machine learning models work with numerical data, we need to encode these categorical variables into numerical formats.\n",
    "\n",
    "#### **Methods for Encoding Categorical Variables**\n",
    "1. **Label Encoding** (`.astype('category').cat.codes`)\n",
    "   - Converts each category into a unique integer.\n",
    "   - Useful for ordinal data (e.g., `Low=0, Medium=1, High=2`).\n",
    "\n",
    "2. **One-Hot Encoding** (`pd.get_dummies()`)\n",
    "   - Creates separate binary columns (0 or 1) for each category.\n",
    "   - Useful for nominal data (e.g., `\"Red\"`, `\"Blue\"`, `\"Green\"`).\n",
    "\n",
    "3. **Ordinal Encoding** (`OrdinalEncoder` from `sklearn.preprocessing`)\n",
    "   - Similar to label encoding but allows specifying a meaningful order.\n",
    "\n",
    "4. **Target Encoding** (Advanced)\n",
    "   - Replaces each category with the mean of the target variable (used in ML).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# Sample DataFrame with categorical variables\n",
    "data = {\n",
    "    'Name': ['Ali', 'Ayesha', 'Hassan', 'Sara', 'Bilal'],\n",
    "    'City': ['Karachi', 'Lahore', 'Islamabad', 'Karachi', 'Lahore'],\n",
    "    'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['City_Label_Encoded'] = label_encoder.fit_transform(df['City'])\n",
    "print(\"\\nDataFrame after Label Encoding:\")\n",
    "print(df)\n",
    "\n",
    "# One-Hot Encoding\n",
    "df_one_hot = pd.get_dummies(df, columns=['City'])\n",
    "print(\"\\nDataFrame after One-Hot Encoding:\")\n",
    "print(df_one_hot)\n",
    "\n",
    "# Ordinal Encoding (Assuming order: Small < Medium < Large)\n",
    "size_categories = [['Small', 'Medium', 'Large']]\n",
    "ordinal_encoder = OrdinalEncoder(categories=size_categories)\n",
    "df['Size_Ordinal_Encoded'] = ordinal_encoder.fit_transform(df[['Size']])\n",
    "print(\"\\nDataFrame after Ordinal Encoding:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is One-Hot Encoding, and When Would You Use It in Data Preprocessing?\n",
    "\n",
    "#### **What is One-Hot Encoding?**\n",
    "One-hot encoding is a technique used to convert categorical variables into a numerical format. It creates **binary (0 or 1) columns** for each unique category in the original feature.\n",
    "\n",
    "For example, if we have a `\"Color\"` column with values `[\"Red\", \"Green\", \"Blue\"]`, one-hot encoding will create separate columns:\n",
    "\n",
    "|     | Red | Green | Blue |\n",
    "|-----|-----|-------|------|\n",
    "| **Red**   | 1   | 0     | 0    |\n",
    "| **Green** | 0   | 1     | 0    |\n",
    "| **Blue**  | 0   | 0     | 1    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **When Should You Use One-Hot Encoding?**\n",
    "- When dealing with **nominal categorical variables** (no inherent order, e.g., `\"Red\"`, `\"Blue\"`, `\"Green\"`).\n",
    "- When preparing data for **machine learning models**, as most models work with numerical inputs.\n",
    "- When categorical variables are **not ordinal**, meaning that the order does not matter.\n",
    "\n",
    "#### **When Should You Avoid One-Hot Encoding?**\n",
    "- When there are **too many unique categories** (e.g., thousands of city names) since it increases dimensionality (curse of dimensionality).\n",
    "- When the variable is **ordinal** (e.g., `\"Small\" < \"Medium\" < \"Large\"`) – use ordinal encoding instead.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with categorical variable\n",
    "data = {\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Red', 'Green']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Applying One-Hot Encoding\n",
    "df_one_hot = pd.get_dummies(df, columns=['Color'])\n",
    "\n",
    "print(\"\\nDataFrame after One-Hot Encoding:\")\n",
    "print(df_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Do You Identify and Remove Duplicate Rows from a DataFrame?\n",
    "\n",
    "#### **Identifying Duplicates**\n",
    "In Pandas, we can check for duplicate rows using:\n",
    "- `df.duplicated()` → Returns a Boolean Series indicating whether each row is a duplicate (`True` for duplicates).\n",
    "- `df.duplicated().sum()` → Counts the total number of duplicate rows.\n",
    "\n",
    "#### **Removing Duplicates**\n",
    "To remove duplicate rows, we use:\n",
    "- `df.drop_duplicates()` → Removes all duplicate rows, keeping only the first occurrence by default.\n",
    "- `df.drop_duplicates(keep='last')` → Keeps the last occurrence instead of the first.\n",
    "- `df.drop_duplicates(subset=['column_name'])` → Removes duplicates based on a specific column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with duplicate rows\n",
    "data = {\n",
    "    'Name': ['Ali', 'Ayesha', 'Hassan', 'Ali', 'Sara'],\n",
    "    'Age': [25, 30, 28, 25, 22],\n",
    "    'City': ['Karachi', 'Lahore', 'Islamabad', 'Karachi', 'Quetta']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identifying duplicate rows\n",
    "print(\"Duplicate Rows (True means duplicate):\")\n",
    "print(df.duplicated())\n",
    "\n",
    "# Counting duplicate rows\n",
    "print(\"\\nTotal Number of Duplicate Rows:\", df.duplicated().sum())\n",
    "\n",
    "# Removing duplicate rows (keeping the first occurrence)\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame after Removing Duplicates:\")\n",
    "print(df_no_duplicates)\n",
    "\n",
    "# Removing duplicates while keeping the last occurrence\n",
    "df_no_duplicates_last = df.drop_duplicates(keep='last')\n",
    "print(\"\\nDataFrame after Removing Duplicates (Keeping Last Occurrence):\")\n",
    "print(df_no_duplicates_last)\n",
    "\n",
    "# Removing duplicates based on a specific column (e.g., 'Name')\n",
    "df_no_duplicates_name = df.drop_duplicates(subset=['Name'])\n",
    "print(\"\\nDataFrame after Removing Duplicates Based on 'Name' Column:\")\n",
    "print(df_no_duplicates_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Between `duplicated()` and `drop_duplicates()` in Pandas\n",
    "\n",
    "#### **1. `duplicated()` Method**\n",
    "- It **identifies** duplicate rows in a DataFrame.\n",
    "- Returns a **Boolean Series**, where `True` indicates a duplicate row.\n",
    "- Does **not** modify the original DataFrame.\n",
    "\n",
    "##### **Syntax:**\n",
    "```python\n",
    "df.duplicated()\n",
    "df.duplicated(subset=['column_name'])\n",
    "df.duplicated(keep='first')  # Default: Marks all but first occurrence as duplicate\n",
    "df.duplicated(keep='last')   # Marks all but last occurrence as duplicate\n",
    "df.duplicated(keep=False)    # Marks all occurrences as duplicate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **Code Cell**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with duplicate rows\n",
    "data = {\n",
    "    'Name': ['Ali', 'Ayesha', 'Hassan', 'Ali', 'Sara', 'Ali'],\n",
    "    'Age': [25, 30, 28, 25, 22, 25],\n",
    "    'City': ['Karachi', 'Lahore', 'Islamabad', 'Karachi', 'Quetta', 'Karachi']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identifying duplicate rows using duplicated()\n",
    "print(\"Duplicate Rows (True means duplicate):\")\n",
    "print(df.duplicated())\n",
    "\n",
    "# Removing duplicates using drop_duplicates()\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame after Removing Duplicates (Keeping First Occurrence):\")\n",
    "print(df_no_duplicates)\n",
    "\n",
    "# Removing duplicates while keeping the last occurrence\n",
    "df_no_duplicates_last = df.drop_duplicates(keep='last')\n",
    "print(\"\\nDataFrame after Removing Duplicates (Keeping Last Occurrence):\")\n",
    "print(df_no_duplicates_last)\n",
    "\n",
    "# Removing all occurrences of duplicates\n",
    "df_no_all_duplicates = df.drop_duplicates(keep=False)\n",
    "print(\"\\nDataFrame after Removing All Duplicate Rows:\")\n",
    "print(df_no_all_duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Feature Scaling in Machine Learning\n",
    "\n",
    "#### **What is Feature Scaling?**\n",
    "Feature scaling is the process of **normalizing or standardizing** numerical data so that all features have a similar range. It ensures that no feature dominates the learning process due to differences in magnitude.\n",
    "\n",
    "#### **Why is Feature Scaling Important?**\n",
    "1. **Improves Model Performance**  \n",
    "   - Algorithms like gradient descent converge faster with scaled features.\n",
    "  \n",
    "2. **Prevents Features with Larger Values from Dominating**  \n",
    "   - ML models work with numerical values, and large-scale differences can bias the model.\n",
    "\n",
    "3. **Essential for Distance-Based Algorithms**  \n",
    "   - Algorithms like **K-Nearest Neighbors (KNN), K-Means Clustering, and Support Vector Machines (SVM)** rely on distances between points, making scaling crucial.\n",
    "\n",
    "4. **Stabilizes Neural Networks**  \n",
    "   - Deep learning models benefit from scaled features, reducing computational costs.\n",
    "\n",
    "5. **Improves Interpretability**  \n",
    "   - Helps in understanding and comparing feature contributions in linear models.\n",
    "\n",
    "#### **Common Feature Scaling Techniques**\n",
    "1. **Min-Max Scaling (Normalization)**\n",
    "   - Rescales values between 0 and 1.\n",
    "   - Formula:  \n",
    "     \\[\n",
    "     X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "     \\]\n",
    "   - Use case: When data needs to be in a fixed range (e.g., neural networks).\n",
    "\n",
    "2. **Standardization (Z-score Normalization)**\n",
    "   - Centers data around mean (0) with standard deviation (1).\n",
    "   - Formula:  \n",
    "     \\[\n",
    "     X' = \\frac{X - \\mu}{\\sigma}\n",
    "     \\]\n",
    "   - Use case: When data follows a normal distribution.\n",
    "\n",
    "3. **Robust Scaling**\n",
    "   - Uses median and interquartile range (IQR) instead of mean and standard deviation.\n",
    "   - Less sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Feature1': [100, 200, 300, 400, 500],\n",
    "    'Feature2': [1, 2, 3, 4, 5],\n",
    "    'Feature3': [1000, 2000, 3000, 4000, 5000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Min-Max Scaling (Normalization)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_min_max = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Standardization (Z-score Normalization)\n",
    "standard_scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Robust Scaling (Handles Outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "df_robust = pd.DataFrame(robust_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Display Results\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nMin-Max Scaled DataFrame:\")\n",
    "print(df_min_max)\n",
    "\n",
    "print(\"\\nStandardized DataFrame:\")\n",
    "print(df_standardized)\n",
    "\n",
    "print(\"\\nRobust Scaled DataFrame:\")\n",
    "print(df_robust)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Between Min-Max Scaling and Z-Score Normalization\n",
    "\n",
    "Feature scaling is an essential step in machine learning, and **Min-Max Scaling** and **Z-Score Normalization (Standardization)** are two common techniques.\n",
    "\n",
    "#### **1. Min-Max Scaling (Normalization)**\n",
    "- Rescales data to a fixed range, usually **[0,1]** or **[-1,1]**.\n",
    "- Formula:\n",
    "  \\[\n",
    "  X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "  \\]\n",
    "- **When to Use?**\n",
    "  - When feature values have a known minimum and maximum.\n",
    "  - When data does **not** follow a normal distribution.\n",
    "  - Useful for algorithms that require bounded input (e.g., **Neural Networks**).\n",
    "\n",
    "#### **2. Z-Score Normalization (Standardization)**\n",
    "- Transforms data to have a **mean of 0** and a **standard deviation of 1**.\n",
    "- Formula:\n",
    "  \\[\n",
    "  X' = \\frac{X - \\mu}{\\sigma}\n",
    "  \\]\n",
    "- **When to Use?**\n",
    "  - When data follows a **normal distribution**.\n",
    "  - When feature values do **not** have a known range.\n",
    "  - Preferred for algorithms like **SVM, K-Means, PCA** that rely on distances.\n",
    "\n",
    "#### **Key Differences**\n",
    "| Method | Range | Affected by Outliers? | Best Used For |\n",
    "|--------|-------|------------------|--------------|\n",
    "| Min-Max Scaling | [0,1] (or [-1,1]) | **Yes** (Sensitive to outliers) | Neural Networks, Image Processing |\n",
    "| Z-Score Normalization | Mean = 0, Std Dev = 1 | **No** (More robust to outliers) | SVM, K-Means, PCA, Linear Regression |\n",
    "\n",
    "#### **Choosing the Right Method**\n",
    "- If your data contains **outliers**, use **Z-Score Normalization**.\n",
    "- If your data needs to be in a **fixed range**, use **Min-Max Scaling**.\n",
    "- Some models like **Tree-based algorithms (Decision Trees, Random Forests)** do **not require** scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50],\n",
    "    'Feature2': [100, 200, 300, 400, 500],\n",
    "    'Feature3': [5, 10, 15, 20, 25]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Applying Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_min_max = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Applying Z-Score Normalization (Standardization)\n",
    "standard_scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Display Results\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nMin-Max Scaled DataFrame:\")\n",
    "print(df_min_max)\n",
    "\n",
    "print(\"\\nStandardized DataFrame (Z-Score Normalization):\")\n",
    "print(df_standardized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Outliers, and Why Might They Impact Machine Learning Models?\n",
    "\n",
    "#### **What Are Outliers?**\n",
    "An **outlier** is a data point that differs significantly from other observations in the dataset. It is an unusually high or low value compared to the rest of the data.\n",
    "\n",
    "#### **Types of Outliers**\n",
    "1. **Univariate Outliers** – Found when analyzing a single feature.\n",
    "2. **Multivariate Outliers** – Detected when considering relationships between multiple features.\n",
    "\n",
    "#### **Why Do Outliers Matter in Machine Learning?**\n",
    "- **Skewing Statistical Measures** – Affects mean, standard deviation, and correlation.\n",
    "- **Impacting Model Performance** – Some ML algorithms (e.g., Linear Regression, K-Means) are sensitive to extreme values.\n",
    "- **Slowing Model Convergence** – Can cause instability in gradient-based models (e.g., Neural Networks).\n",
    "- **Misleading Insights** – Can result in incorrect conclusions or poor model predictions.\n",
    "\n",
    "#### **When to Remove or Keep Outliers?**\n",
    "- **Remove outliers** if they are due to errors, sensor malfunctions, or irrelevant noise.\n",
    "- **Keep outliers** if they provide valuable insights (e.g., fraud detection, rare events).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample Data with an Outlier\n",
    "data = {'Feature1': [10, 12, 14, 16, 18, 500]}  # 500 is an outlier\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Boxplot to Visualize Outliers\n",
    "plt.boxplot(df['Feature1'])\n",
    "plt.title(\"Boxplot for Outlier Detection\")\n",
    "plt.show()\n",
    "\n",
    "# Identifying Outliers using Z-Score\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df['Z-Score'] = np.abs(zscore(df['Feature1']))\n",
    "outliers = df[df['Z-Score'] > 3]  # Outliers have Z-score > 3\n",
    "\n",
    "print(\"\\nOutliers Detected Using Z-Score:\")\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Methods for Detecting Outliers in a Dataset\n",
    "\n",
    "Outliers are extreme values that differ significantly from the rest of the data. Detecting and handling them is crucial for improving the performance of machine learning models.\n",
    "\n",
    "#### **1. Z-Score Method (Standard Deviation)**\n",
    "- Measures how many standard deviations a data point is from the mean.\n",
    "- A common threshold for outliers is **Z-score > 3 or < -3**.\n",
    "- Best for normally distributed data.\n",
    "\n",
    "#### **2. IQR (Interquartile Range) Method**\n",
    "- Uses the **first quartile (Q1) and third quartile (Q3)** to define outliers.\n",
    "- Data points outside the range **[Q1 - 1.5 × IQR, Q3 + 1.5 × IQR]** are considered outliers.\n",
    "- Works well with skewed data.\n",
    "\n",
    "#### **3. Boxplot Method**\n",
    "- A graphical approach using boxplots to visualize outliers.\n",
    "- Outliers appear as **individual points outside the whiskers**.\n",
    "\n",
    "#### **4. Isolation Forest**\n",
    "- A machine learning model that isolates anomalies using decision trees.\n",
    "- Assigns an \"anomaly score\" to each observation.\n",
    "\n",
    "#### **5. DBSCAN (Density-Based Clustering)**\n",
    "- Detects outliers by identifying points in low-density areas.\n",
    "- Good for **unsupervised anomaly detection**.\n",
    "\n",
    "Each method has its advantages, and the choice depends on the data distribution and the problem at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Sample Data\n",
    "data = {'Feature1': [10, 12, 14, 16, 18, 500]}  # 500 is an outlier\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Z-Score Method\n",
    "df['Z-Score'] = np.abs(zscore(df['Feature1']))\n",
    "outliers_z = df[df['Z-Score'] > 3]\n",
    "\n",
    "# 2. IQR Method\n",
    "Q1 = df['Feature1'].quantile(0.25)\n",
    "Q3 = df['Feature1'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = df[(df['Feature1'] < Q1 - 1.5 * IQR) | (df['Feature1'] > Q3 + 1.5 * IQR)]\n",
    "\n",
    "# 3. Boxplot Visualization\n",
    "plt.boxplot(df['Feature1'])\n",
    "plt.title(\"Boxplot for Outlier Detection\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "df['Anomaly Score'] = iso_forest.fit_predict(df[['Feature1']])\n",
    "outliers_iso = df[df['Anomaly Score'] == -1]\n",
    "\n",
    "# Display Results\n",
    "print(\"\\nOutliers Detected Using Z-Score:\")\n",
    "print(outliers_z)\n",
    "\n",
    "print(\"\\nOutliers Detected Using IQR Method:\")\n",
    "print(outliers_iqr)\n",
    "\n",
    "print(\"\\nOutliers Detected Using Isolation Forest:\")\n",
    "print(outliers_iso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Handle Outliers in a Continuous Numerical Variable\n",
    "\n",
    "Outliers can distort statistical analysis and negatively impact machine learning models. Here are several methods to handle outliers in continuous numerical variables:\n",
    "\n",
    "#### **1. Removal of Outliers**\n",
    "- If outliers are due to errors or irrelevant data, they can be removed.\n",
    "- **Methods:**\n",
    "  - Z-Score (Remove values with |Z| > 3)\n",
    "  - IQR Method (Remove values outside **[Q1 - 1.5 × IQR, Q3 + 1.5 × IQR]**)\n",
    "\n",
    "#### **2. Winsorization (Capping)**\n",
    "- Replace extreme values with a specified percentile (e.g., 5th and 95th percentiles).\n",
    "\n",
    "#### **3. Transformation Techniques**\n",
    "- Apply transformations like **log, square root, or Box-Cox** to reduce the effect of outliers.\n",
    "\n",
    "#### **4. Binning**\n",
    "- Convert continuous variables into categorical bins to reduce the impact of extreme values.\n",
    "\n",
    "#### **5. Model-Based Approaches**\n",
    "- Use **Robust Regression** (e.g., RANSAC) or **Tree-Based Models** (e.g., Decision Trees, Random Forests) that are less sensitive to outliers.\n",
    "\n",
    "The best method depends on whether the outliers are **genuine data points** or **errors**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Sample Data\n",
    "data = {'Feature1': [10, 12, 14, 16, 18, 500]}  # 500 is an outlier\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Removing Outliers Using Z-Score\n",
    "df_zscore = df[np.abs(zscore(df['Feature1'])) < 3]\n",
    "\n",
    "# 2. Removing Outliers Using IQR Method\n",
    "Q1 = df['Feature1'].quantile(0.25)\n",
    "Q3 = df['Feature1'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df_iqr = df[(df['Feature1'] >= Q1 - 1.5 * IQR) & (df['Feature1'] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "# 3. Winsorization (Capping Outliers)\n",
    "lower_limit = df['Feature1'].quantile(0.05)  # 5th percentile\n",
    "upper_limit = df['Feature1'].quantile(0.95)  # 95th percentile\n",
    "df_winsorized = df.copy()\n",
    "df_winsorized['Feature1'] = np.clip(df['Feature1'], lower_limit, upper_limit)\n",
    "\n",
    "# 4. Log Transformation\n",
    "df_log_transformed = df.copy()\n",
    "df_log_transformed['Feature1'] = np.log1p(df['Feature1'])  # log1p avoids log(0) issues\n",
    "\n",
    "# Display Results\n",
    "print(\"\\nOriginal Data:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nAfter Removing Outliers (Z-Score):\")\n",
    "print(df_zscore)\n",
    "\n",
    "print(\"\\nAfter Removing Outliers (IQR Method):\")\n",
    "print(df_iqr)\n",
    "\n",
    "print(\"\\nAfter Winsorization (Capping Outliers):\")\n",
    "print(df_winsorized)\n",
    "\n",
    "print(\"\\nAfter Log Transformation:\")\n",
    "print(df_log_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
